<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Offline analysis &mdash; pupil_recording_interface 0.5.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="_static/sg_gallery-rendered-html.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/copybutton.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Additional features" href="additional.html" />
    <link rel="prev" title="Processing and recording data" href="processing.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            pupil_recording_interface
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
</ul>
<p class="caption"><span class="caption-text">User guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="reading.html">Reading data</a></li>
<li class="toctree-l1"><a class="reference internal" href="streaming.html">Streaming data</a></li>
<li class="toctree-l1"><a class="reference internal" href="processing.html">Processing and recording data</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Offline analysis</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#pupil-detection">Pupil detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="#marker-detection">Marker detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="#calibration">Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gaze-mapping">Gaze mapping</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="additional.html">Additional features</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom.html">Custom devices, streams and processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_examples/index.html">Examples</a></li>
</ul>
<p class="caption"><span class="caption-text">Help &amp; reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="whatsnew.html">What’s New</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">pupil_recording_interface</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Offline analysis</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/analysis.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="offline-analysis">
<span id="analysis"></span><h1>Offline analysis<a class="headerlink" href="#offline-analysis" title="Permalink to this headline"></a></h1>
<p>pupil_recording_interface provides straightforward interfaces for post-hoc
pupil detection, calibration and gaze mapping, similar to Pupil Player.</p>
<section id="pupil-detection">
<h2>Pupil detection<a class="headerlink" href="#pupil-detection" title="Permalink to this headline"></a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Make sure that you have installed the necessary
<a class="reference internal" href="installation.html#pupil-detection-dependencies"><span class="std std-ref">dependencies for pupil detection</span></a>.</p>
</div>
<p>We start by creating a <a class="reference internal" href="_generated/pupil_recording_interface.PupilDetector.html#pupil_recording_interface.PupilDetector" title="pupil_recording_interface.PupilDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">PupilDetector</span></code></a> in the same way as in the
<a class="reference internal" href="processing.html#processing"><span class="std std-ref">processing tutorial</span></a>.</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pupil_recording_interface</span> <span class="k">as</span> <span class="nn">pri</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pupil_detector</span> <span class="o">=</span> <span class="n">pri</span><span class="o">.</span><span class="n">PupilDetector</span><span class="p">(</span><span class="n">camera_id</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>For post-hoc pupil detection we first create a <a class="reference internal" href="_generated/pupil_recording_interface.VideoReader.html#pupil_recording_interface.VideoReader" title="pupil_recording_interface.VideoReader"><code class="xref py py-class docutils literal notranslate"><span class="pre">VideoReader</span></code></a> instance
that points to an eye camera video. Now we can use the
<a class="reference internal" href="_generated/pupil_recording_interface.PupilDetector.html#pupil_recording_interface.PupilDetector.batch_run" title="pupil_recording_interface.PupilDetector.batch_run"><code class="xref py py-meth docutils literal notranslate"><span class="pre">PupilDetector.batch_run()</span></code></a> method, which will return a list containing
pupil data for each frame:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">eye0_reader</span> <span class="o">=</span> <span class="n">pri</span><span class="o">.</span><span class="n">VideoReader</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">pri</span><span class="o">.</span><span class="n">get_test_recording</span><span class="p">(),</span> <span class="n">stream</span><span class="o">=</span><span class="s2">&quot;eye0&quot;</span><span class="p">,</span> <span class="n">color_format</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pupil_list_eye0</span> <span class="o">=</span> <span class="n">pupil_detector</span><span class="o">.</span><span class="n">batch_run</span><span class="p">(</span><span class="n">eye0_reader</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pupil_list_eye0</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 
<span class="go">{&#39;ellipse&#39;:</span>
<span class="go">    {&#39;center&#39;: (96..., 130...),</span>
<span class="go">     &#39;axes&#39;: (39..., 44...),</span>
<span class="go">     &#39;angle&#39;: 77...},</span>
<span class="go"> &#39;diameter&#39;: 44...,</span>
<span class="go"> &#39;location&#39;: (96..., 130...),</span>
<span class="go"> &#39;confidence&#39;: 0.99,</span>
<span class="go"> &#39;internal_2d_raw_data&#39;: ...,</span>
<span class="go"> &#39;norm_pos&#39;: (0.5..., 0.6...),</span>
<span class="go"> &#39;timestamp&#39;: 2294...,</span>
<span class="go"> &#39;method&#39;: &#39;2d c++&#39;,</span>
<span class="go"> &#39;id&#39;: 0,</span>
<span class="go"> &#39;topic&#39;: &#39;pupil&#39;}</span>
</pre></div>
</div>
<p>We can also tell the detector to stop after 100 frames and return a dataset:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pupil_detector</span><span class="o">.</span><span class="n">batch_run</span><span class="p">(</span><span class="n">eye0_reader</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">)</span>
<span class="go">&lt;xarray.Dataset&gt;</span>
<span class="go">Dimensions:         (pixel_axis: 2, time: 100)</span>
<span class="go">Coordinates:</span>
<span class="go">  * time            (time) datetime64[ns] 2019-10-10T16:43:20.280291796 ... 2019-10-10T16:43:21.079125643</span>
<span class="go">  * pixel_axis      (pixel_axis) &lt;U1 &#39;x&#39; &#39;y&#39;</span>
<span class="go">Data variables:</span>
<span class="go">    eye             (time) int64 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0</span>
<span class="go">    confidence      (time) float64 0.99 0.9674 0.924 ... 0.9381 0.9711 0.9554</span>
<span class="go">    diameter        (time) float64 44.2 44.2 44.37 44.35 ... 45.75 45.8 45.79</span>
<span class="go">    ellipse_angle   (time) float64 77.64 78.05 76.35 77.67 ... 78.78 80.25 79.18</span>
<span class="go">    pupil_norm_pos  (time, pixel_axis) float64 0.5005 0.6779 ... 0.5067 0.677</span>
<span class="go">    ellipse_center  (time, pixel_axis) float64 96.1 130.1 96.13 ... 97.29 130.0</span>
<span class="go">    ellipse_axes    (time, pixel_axis) float64 39.78 44.2 39.68 ... 41.15 45.79</span>
</pre></div>
</div>
</section>
<section id="marker-detection">
<h2>Marker detection<a class="headerlink" href="#marker-detection" title="Permalink to this headline"></a></h2>
<p>To detect calibration markers in the world camera images, we can use the
<a class="reference internal" href="_generated/pupil_recording_interface.CircleDetector.html#pupil_recording_interface.CircleDetector" title="pupil_recording_interface.CircleDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">CircleDetector</span></code></a>:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">marker_detector</span> <span class="o">=</span> <span class="n">pri</span><span class="o">.</span><span class="n">CircleDetector</span><span class="p">()</span>
</pre></div>
</div>
<p>This process also has a <a class="reference internal" href="_generated/pupil_recording_interface.CircleDetector.html#pupil_recording_interface.CircleDetector.batch_run" title="pupil_recording_interface.CircleDetector.batch_run"><code class="xref py py-meth docutils literal notranslate"><span class="pre">CircleDetector.batch_run()</span></code></a> method, to which
we pass a <a class="reference internal" href="_generated/pupil_recording_interface.VideoReader.html#pupil_recording_interface.VideoReader" title="pupil_recording_interface.VideoReader"><code class="xref py py-class docutils literal notranslate"><span class="pre">VideoReader</span></code></a> for the world video:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">world_reader</span> <span class="o">=</span> <span class="n">pri</span><span class="o">.</span><span class="n">VideoReader</span><span class="p">(</span><span class="n">pri</span><span class="o">.</span><span class="n">get_test_recording</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">marker_list</span> <span class="o">=</span> <span class="n">marker_detector</span><span class="o">.</span><span class="n">batch_run</span><span class="p">(</span><span class="n">world_reader</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">marker_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 
<span class="go">{&#39;ellipses&#39;: [((202..., 258...), (3..., 7...), 5...),</span>
<span class="go">              ((202..., 258...), (9..., 14...), 7...),</span>
<span class="go">              ((202..., 258...), (14..., 21...), 8...)],</span>
<span class="go"> &#39;img_pos&#39;: (202..., 258...),</span>
<span class="go"> &#39;norm_pos&#39;: (0.1..., 0.6...),</span>
<span class="go"> &#39;marker_type&#39;: &#39;Ref&#39;,</span>
<span class="go"> &#39;timestamp&#39;: 2294...,</span>
<span class="go"> &#39;frame_index&#39;: 0}</span>
</pre></div>
</div>
<p>Again, we can specify a stop (and start) index and return a dataset:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">marker_detector</span><span class="o">.</span><span class="n">batch_run</span><span class="p">(</span><span class="n">world_reader</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">)</span>
<span class="go">&lt;xarray.Dataset&gt;</span>
<span class="go">Dimensions:      (pixel_axis: 2, time: 34)</span>
<span class="go">Coordinates:</span>
<span class="go">  * time         (time) datetime64[ns] 2019-10-10T16:43:22.248995781 ... 2019-10-10T16:43:23.472124815</span>
<span class="go">  * pixel_axis   (pixel_axis) &lt;U1 &#39;x&#39; &#39;y&#39;</span>
<span class="go">Data variables:</span>
<span class="go">    frame_index  (time) int64 66 67 68 69 70 71 72 73 ... 93 94 95 96 97 98 99</span>
<span class="go">    location     (time, pixel_axis) float64 619.1 352.2 618.9 ... 615.7 354.0</span>
</pre></div>
</div>
</section>
<section id="calibration">
<h2>Calibration<a class="headerlink" href="#calibration" title="Permalink to this headline"></a></h2>
<p>After detecting pupils and calibration markers, we can now run a calibration
with a <a class="reference internal" href="_generated/pupil_recording_interface.Calibration.html#pupil_recording_interface.Calibration" title="pupil_recording_interface.Calibration"><code class="xref py py-class docutils literal notranslate"><span class="pre">Calibration</span></code></a> instance:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">calibration</span> <span class="o">=</span> <span class="n">pri</span><span class="o">.</span><span class="n">Calibration</span><span class="p">(</span><span class="n">resolution</span><span class="o">=</span><span class="p">(</span><span class="mi">1280</span><span class="p">,</span> <span class="mi">720</span><span class="p">))</span>
</pre></div>
</div>
<p>We pass the detected pupils and calibration markers to
<a class="reference internal" href="_generated/pupil_recording_interface.Calibration.html#pupil_recording_interface.Calibration.batch_run" title="pupil_recording_interface.Calibration.batch_run"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Calibration.batch_run()</span></code></a> to obtain a monocular calibration.</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">calibration</span><span class="o">.</span><span class="n">batch_run</span><span class="p">(</span><span class="n">pupil_list_eye0</span><span class="p">,</span> <span class="n">marker_list</span><span class="p">)</span> 
<span class="go">{&#39;params&#39;: ([-28..., -29..., 15..., 14..., 41..., -29..., 13...],</span>
<span class="go">            [-2..., -14..., 2..., 13..., -0..., 0..., 4...],</span>
<span class="go">            7)}</span>
</pre></div>
</div>
<p>It is also possible to perform a binocular calibration, provided that we have
detected pupils from the second eye camera. However, we need to take care that
the detected pupils are in the correct chronological order. We can use
<a class="reference internal" href="_generated/pupil_recording_interface.merge_pupils.html#pupil_recording_interface.merge_pupils" title="pupil_recording_interface.merge_pupils"><code class="xref py py-meth docutils literal notranslate"><span class="pre">merge_pupils()</span></code></a> for this:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">eye1_reader</span> <span class="o">=</span> <span class="n">pri</span><span class="o">.</span><span class="n">VideoReader</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">pri</span><span class="o">.</span><span class="n">get_test_recording</span><span class="p">(),</span> <span class="n">stream</span><span class="o">=</span><span class="s2">&quot;eye1&quot;</span><span class="p">,</span> <span class="n">color_format</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pupil_detector</span><span class="o">.</span><span class="n">camera_id</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pupil_list_eye1</span> <span class="o">=</span> <span class="n">pupil_detector</span><span class="o">.</span><span class="n">batch_run</span><span class="p">(</span><span class="n">eye1_reader</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pupil_list</span> <span class="o">=</span> <span class="n">pri</span><span class="o">.</span><span class="n">merge_pupils</span><span class="p">(</span><span class="n">pupil_list_eye0</span><span class="p">,</span> <span class="n">pupil_list_eye1</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we can run a binocular calibration. As you can see, the result is different
this time, as it includes binocular calibration coefficients as well as
monocular coefficients for each eye:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">calibration_result</span> <span class="o">=</span> <span class="n">calibration</span><span class="o">.</span><span class="n">batch_run</span><span class="p">(</span><span class="n">pupil_list</span><span class="p">,</span> <span class="n">marker_list</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">calibration_result</span> 
<span class="go">{&#39;params&#39;: ([-1..., -12..., -2..., 1..., -0..., 8..., 5..., -2..., 1...,</span>
<span class="go">             -6..., 1..., 5..., 5...],</span>
<span class="go">            [8..., 16..., 4..., 7..., -4..., -12..., -13..., 13..., -3...,</span>
<span class="go">             -8..., -10..., 17..., -6...], 13),</span>
<span class="go"> &#39;params_eye0&#39;: ([-28..., -29..., 15..., 14..., 41..., -29..., 13...],</span>
<span class="go">                 [2..., 14..., -2..., -13..., 0..., -0..., -3...], 7),</span>
<span class="go"> &#39;params_eye1&#39;: ([-3..., -0..., 2..., -1..., 3..., -0..., 1...],</span>
<span class="go">                 [1..., -2..., -1..., 9..., -1..., 6..., 0...], 7)}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Running a calibration on pupil and marker datasets is not yet possible.</p>
</div>
</section>
<section id="gaze-mapping">
<h2>Gaze mapping<a class="headerlink" href="#gaze-mapping" title="Permalink to this headline"></a></h2>
<p>Finally, we can use the <a class="reference internal" href="_generated/pupil_recording_interface.GazeMapper.html#pupil_recording_interface.GazeMapper" title="pupil_recording_interface.GazeMapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">GazeMapper</span></code></a> to map detected pupils to gaze
data according to a previously obtained calibration:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gaze_mapper</span> <span class="o">=</span> <span class="n">pri</span><span class="o">.</span><span class="n">GazeMapper</span><span class="p">(</span><span class="n">calibration</span><span class="o">=</span><span class="n">calibration_result</span><span class="p">)</span>
</pre></div>
</div>
<p>As usual, we use the <a class="reference internal" href="_generated/pupil_recording_interface.GazeMapper.html#pupil_recording_interface.GazeMapper.batch_run" title="pupil_recording_interface.GazeMapper.batch_run"><code class="xref py py-meth docutils literal notranslate"><span class="pre">GazeMapper.batch_run()</span></code></a> method for post-hoc
mapping:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gaze_list</span> <span class="o">=</span> <span class="n">gaze_mapper</span><span class="o">.</span><span class="n">batch_run</span><span class="p">(</span><span class="n">pupil_list</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gaze_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 
<span class="go">{&#39;topic&#39;: &#39;gaze.2d.01.&#39;,</span>
<span class="go">  &#39;norm_pos&#39;: (0.33..., 0.67...),</span>
<span class="go">  &#39;confidence&#39;: 0.97...,</span>
<span class="go">  &#39;timestamp&#39;: 2294...,</span>
<span class="go">  &#39;base_data&#39;: [...]}</span>
</pre></div>
</div>
<p>The method can also return a dataset, although we need to provide a dictionary
with the recording info to get the correspondence between recorded (monotonic)
timestamps and datetime timestamps:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">info</span> <span class="o">=</span> <span class="n">pri</span><span class="o">.</span><span class="n">load_info</span><span class="p">(</span><span class="n">pri</span><span class="o">.</span><span class="n">get_test_recording</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gaze_mapper</span><span class="o">.</span><span class="n">batch_run</span><span class="p">(</span><span class="n">pupil_list</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span> <span class="n">info</span><span class="o">=</span><span class="n">info</span><span class="p">)</span>
<span class="go">&lt;xarray.Dataset&gt;</span>
<span class="go">Dimensions:             (pixel_axis: 2, time: 5154)</span>
<span class="go">Coordinates:</span>
<span class="go">  * time                (time) datetime64[ns] 2019-10-10T16:43:20.278882504 ... 2019-10-10T16:43:41.326934099</span>
<span class="go">  * pixel_axis          (pixel_axis) &lt;U1 &#39;x&#39; &#39;y&#39;</span>
<span class="go">Data variables:</span>
<span class="go">    eye                 (time) int64 2 2 2 2 2 2 2 2 2 2 ... 2 2 2 2 2 2 2 2 2 2</span>
<span class="go">    gaze_norm_pos       (time, pixel_axis) float64 0.3359 0.6786 ... 0.9603</span>
<span class="go">    gaze_confidence_2d  (time) float64 0.9761 0.9586 0.9473 ... 1.0 0.984 0.984</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Running gaze mapping on a pupil dataset is not yet possible.</p>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="processing.html" class="btn btn-neutral float-left" title="Processing and recording data" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="additional.html" class="btn btn-neutral float-right" title="Additional features" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Peter Hausamann / The Visual Experience Database.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>